{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiLHNB4jqHhb",
    "outputId": "c5b029ce-6e1f-4a63-cd77-316b21627316",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gpt_index\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4rCtGuDorhfW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "from langchain import OpenAI\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xFX7kdXpsUnv"
   },
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzM0PNIquXGC",
    "outputId": "4b5d24cd-84d9-4b1b-fb49-ae51ca1908f6"
   },
   "outputs": [],
   "source": [
    "!wget !wget https://raw.githubusercontent.com/GITenberg/Meditations_2680/master/2680.txt -O text.txt #http://classics.mit.edu/Antoninus/meditations.mb.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T1bzzjGVsq-g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_index(directory_path):\n",
    "  max_input_size=4096\n",
    "  num_output=256\n",
    "  max_chunk_overlap=20\n",
    "  chunk_size_limit=600\n",
    "\n",
    "  prompt_helper=PromptHelper(max_input_size,num_output,max_chunk_overlap,chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "  #define LLM\n",
    "  llm_predictor=LLMPredictor(llm=OpenAI(temperature=0,model_name=\"text-ada-001\",max_tokens=num_output))\n",
    "\n",
    "  documents=SimpleDirectoryReader(directory_path).load_data()\n",
    "  print(documents)\n",
    "  #index=GPTSimpleVectorIndex(documents,llm_predictor=llm_predictor,prompt_helpler=prompt_helper)\n",
    "  index=GPTSimpleVectorIndex.from_documents(documents=documents)\n",
    "  index.save_to_disk('index.json')\n",
    "\n",
    "def ask_bot(input_index='index.json'):\n",
    "  index=GPTSimpleVectorIndex.load_from_disk(input_index)\n",
    "  while True:\n",
    "    query=input('what do you want to ask the robot?')\n",
    "    response=index.query(query,response_mode='compact')\n",
    "    print(\"\\nBot said:\\n\\n\"+response.response+\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H17jeZsfwQXY",
    "outputId": "4fe1c44b-6718-44e8-8b7c-6422816a5d77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "index=construct_index('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ2n1l9J3xdU",
    "outputId": "09115864-b18e-41aa-bd69-566352ac0ac9"
   },
   "outputs": [],
   "source": [
    "ask_bot('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
